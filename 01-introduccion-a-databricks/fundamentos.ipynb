{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e09ceb-cefc-4a4d-b2ed-8118fbc4ed55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sesión 1: Introducción a Databricks y Fundamentos\n",
    "\n",
    "**Curso:** Introducción Práctica a Databricks: Ingeniería, Ciencia de Datos y MLflow  \n",
    "**Duración de la sesión:** 2 horas  \n",
    "**Fecha de preparación del material:** 2025-10-16\n",
    "\n",
    "Este notebook está diseñado para ser usado en un entorno Databricks Premium Trial sobre AWS. Incluye explicaciones teóricas, ejemplos prácticos (Python y SQL con magics de Databricks) y secciones de síntesis.\n",
    "\n",
    "---\n",
    "## Objetivos de aprendizaje\n",
    "- Comprender qué es Databricks y su relación con Apache Spark.\n",
    "- Diferenciar Data Lakes, Data Warehouses y el enfoque Lakehouse.\n",
    "- Reconocer casos de uso empresariales típicos.\n",
    "- Familiarizarse con el Workspace de Databricks, los notebooks y la configuración básica de clústeres.\n",
    "- Ejecutar consultas simples usando PySpark y Databricks SQL.\n",
    "\n",
    "---\n",
    "## Agenda\n",
    "1. ¿Qué es Databricks? Historia y relación con Apache Spark.\n",
    "2. Diferencia entre Data Lakes, Data Warehouses y Lakehouse.\n",
    "3. Casos de uso en la industria.\n",
    "4. Entorno de trabajo en Databricks: Workspace, notebooks y clústeres.\n",
    "5. Workshop: primeros pasos (Python y SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae09465-9d87-4d8d-9247-e836b0d6eb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¿Qué es Databricks?\n",
    "Databricks es una plataforma unificada de datos y analítica basada en Apache Spark que permite construir, ejecutar y escalar cargas de trabajo de ingeniería de datos, ciencia de datos y aprendizaje automático en la nube. Provee un entorno colaborativo con notebooks, gestión de clústeres, orquestación de trabajos y un **Lakehouse** que integra las fortalezas del Data Lake y el Data Warehouse.\n",
    "\n",
    "### Historia y relación con Apache Spark\n",
    "- Apache Spark surge en la Universidad de California, Berkeley (AMPLab) como un motor de procesamiento distribuido de propósito general.\n",
    "- Databricks fue fundado por los creadores originales de Spark para simplificar su adopción empresarial y llevarlo a escala en la nube.\n",
    "- La plataforma ofrece **runtimes optimizados** (Databricks Runtime), mejoras de performance, conectores nativos y herramientas de colaboración.\n",
    "\n",
    "### Cosas para recordar\n",
    "- Databricks = plataforma unificada centrada en Spark + herramientas de colaboración y gobierno de datos.\n",
    "- Se ejecuta sobre proveedores cloud (AWS en este curso), aprovechando servicios como S3 e IAM.\n",
    "- Lakehouse como enfoque arquitectónico nativo.\n",
    "\n",
    "### Importancia en Ingeniería de datos\n",
    "- Permite diseñar pipelines reproducibles, con calidad y gobernanza.\n",
    "- Facilita el manejo de grandes volúmenes y variedad de datos con rendimiento distribuido.\n",
    "- Reduce fricción entre equipos (datos, analítica, ML) al unificar herramientas.\n",
    "\n",
    "### Conclusiones\n",
    "Databricks acelera la entrega de valor en analítica y ML al ofrecer una base unificada y escalable sobre Spark, integrada con servicios cloud y prácticas de ingeniería modernas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b4429d-f472-4e41-99e6-f98443dfbe21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Diferencia entre Data Lakes, Data Warehouses y Lakehouse\n",
    "### Data Lake\n",
    "- Almacén de datos en bruto (raw) y semiestructurados/estructurados en almacenamiento de objetos (ej. S3).\n",
    "- Alta escalabilidad y costo eficiente; esquema a la lectura (schema-on-read).\n",
    "- Ideal para ingestión masiva y exploración.\n",
    "\n",
    "### Data Warehouse\n",
    "- Repositorio optimizado para consultas analíticas con esquema definido (schema-on-write).\n",
    "- Ofrece gobernanza estricta, rendimiento de consulta y BI tradicional.\n",
    "\n",
    "### Lakehouse\n",
    "- Combina lo mejor de ambos: flexibilidad del Lake + gobernanza y rendimiento del Warehouse.\n",
    "- **Delta Lake** aporta transacciones ACID, versionado, time travel e índices/optimizaciones.\n",
    "\n",
    "#### Esquema conceptual (alto nivel)\n",
    "```\n",
    "[Fuentes] -> [Data Lake (S3 + Delta)] -> [Tablas / Vistas] -> [BI/ML]\n",
    "                         | ACID, time travel, DQ |\n",
    "```\n",
    "\n",
    "### Cosas para recordar\n",
    "- Lakehouse reduce silos y duplicidad de arquitecturas.\n",
    "- Delta Lake habilita confiabilidad y rendimiento en el lago.\n",
    "- Permite analítica avanzada y ML sobre los mismos datos gobernados.\n",
    "\n",
    "### Importancia en Ingeniería de datos\n",
    "- Simplifica pipelines y minimiza movimientos innecesarios de datos.\n",
    "- Estándar de tabla abierto (Delta) que favorece interoperabilidad.\n",
    "- Mejora la calidad de datos con auditoría y versionado.\n",
    "\n",
    "### Conclusiones\n",
    "Adoptar un Lakehouse con Delta Lake ofrece una base robusta para casos de BI y ML con menor complejidad operativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c650ef28-5c5a-40dc-b9be-5e8adab2dd2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Casos de uso en la industria\n",
    "- **Ingesta y modelado analítico**: Integración de datos de ERP/CRM, construcción de tablas de hechos y dimensiones.\n",
    "- **Streaming y near real-time**: Procesamiento de eventos (logs, IoT) con Structured Streaming.\n",
    "- **Machine Learning a escala**: Entrenamiento y scoring distribuido; administración de modelos con MLflow.\n",
    "- **Data Sharing / Colaboración**: Intercambio seguro de datos con partners; catálogos gobernados.\n",
    "- **MLOps/Medición de impacto**: Versionado de datasets/modelos, seguimiento de métricas.\n",
    "\n",
    "### Cosas para recordar\n",
    "- La plataforma soporta batch, streaming y ML en una misma superficie.\n",
    "- Integración con ecosistema AWS (S3, IAM, Glue Catalog) y conectores JDBC.\n",
    "\n",
    "### Importancia en Ingeniería de datos\n",
    "- Estándar de facto para construir pipelines resilientes y escalables.\n",
    "- Acelera prototipado y puesta en producción.\n",
    "\n",
    "### Conclusiones\n",
    "La unificación en Databricks habilita flujos de punta a punta, desde la ingesta hasta el consumo en BI/ML, con gobierno y desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19499a65-4776-4e55-9edb-c6f77c78d9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Entorno de Trabajo en Databricks\n",
    "### Navegación en el Workspace\n",
    "Componentes principales:\n",
    "- **Workspace**: organiza notebooks, repositorios, librerías y dashboards.\n",
    "- **Compute**: creación y administración de clústeres; políticas y autoscaling.\n",
    "- **Data**: exploración de catálogos, bases y tablas (incluye Delta Lake).\n",
    "- **Workflows (Jobs)**: orquestación y programación de pipelines.\n",
    "- **Repos**: integración con Git (GitHub) para versionado colaborativo.\n",
    "\n",
    "### Notebooks y lenguajes soportados\n",
    "- Notebooks multicelda con soporte para **Python, SQL, Scala y R**.\n",
    "- Uso de *magics* para cambiar de lenguaje en una celda (ej. `%sql`, `%python`).\n",
    "- Integración con visualizaciones, widgets y documentación Markdown.\n",
    "\n",
    "### Configuración básica de clústeres\n",
    "Pasos recomendados (AWS, Premium Trial):\n",
    "1. Seleccionar **Databricks Runtime** estable (Spark 3.x).\n",
    "2. Elegir tipo y tamaño de nodos dentro de los límites del trial.\n",
    "3. Habilitar **autoscaling** y política de auto-terminación para optimizar costos.\n",
    "4. (Opcional) Configurar acceso a S3 mediante IAM roles o External Locations.\n",
    "\n",
    "### Cosas para recordar\n",
    "- Los notebooks se ejecutan sobre un clúster; sin clúster no hay ejecución.\n",
    "- Autoscaling y auto-terminación ayudan a controlar costos.\n",
    "- Repos + GitHub = versionado y colaboración.\n",
    "\n",
    "### Importancia en Ingeniería de datos\n",
    "- Administración correcta del cómputo asegura rendimiento y eficiencia.\n",
    "- Organización del Workspace y repos mejora la mantenibilidad.\n",
    "\n",
    "### Conclusiones\n",
    "Un Workspace bien organizado y clústeres optimizados son la base operativa para pipelines confiables y eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42bd88e7-e324-4255-9615-43b568d62d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workshop: Primeros pasos\n",
    "En esta sección crearemos y ejecutaremos celdas en Python y SQL, y realizaremos consultas simples.\n",
    "\n",
    "### 1) Creación de un notebook y ejecución de celdas\n",
    "En Databricks:\n",
    "1. Ir a **Workspace** → **Create** → **Notebook**.\n",
    "2. Asignar un nombre descriptivo.\n",
    "3. Seleccionar el **lenguaje por defecto** (Python recomendado).\n",
    "4. Adjuntar un **clúster** disponible.\n",
    "5. Ejecutar celdas con **Shift+Enter**.\n",
    "\n",
    "A continuación, ejecutaremos algunas celdas de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c7d135-fde2-45b8-ac18-09cfe6f98d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- (Opcional) Verifica el contexto actual\n",
    "SELECT current_catalog(), current_schema();\n",
    "\n",
    "-- Elimina la BD y todos sus objetos (tablas, vistas) en cascada\n",
    "DROP SCHEMA IF EXISTS demo_db_jengrik CASCADE;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01e6aba-ad8d-4b9e-90ae-0afead1ff054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Python: Comprobación del entorno\n",
    "import sys\n",
    "print(\"Versión de Python: \", sys.version)\n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"Pyspark: \", pyspark.__version__, \"- Todo OK.\")\n",
    "except Exception as e:\n",
    "    print(\"Pyspark no disponible en este entorno.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9346a2-2579-478e-a461-737fdb00a826",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760715062282}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PySpark: Crear un DataFrame sencillo y operaciones básicas\n",
    "from pyspark.sql import functions as F\n",
    "df = spark.range(0 ,11).withColumn('par', (F.col('id') % 2 == 0).cast('boolean'))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df72a1b9-f6b1-4848-ae0b-749d36595200",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760715341900}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_agg = df.groupby('par').agg(F.count('*').alias('count'))\n",
    "display(df_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3cf498-da4d-40af-8b96-26ac70a57f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Introducción a Databricks SQL\n",
    "Databricks SQL permite consultas interactivas y creación de dashboards sobre tablas gestionadas en el Lakehouse. Desde notebooks, podemos usar celdas con **`%sql`** para ejecutar SQL directamente.\n",
    "\n",
    "A continuación, demostraremos consultas SQL. Si su workspace no contiene las tablas de ejemplo mencionadas, puede crear una tabla temporal o ajustar los nombres según su catálogo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a482379-18c6-4e08-8ba5-6b038bdbd517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Exploración del catálogo\n",
    "CREATE OR REPLACE TEMP VIEW trips AS\n",
    "SELECT * FROM VALUES\n",
    " (1), (2), (1), (3), (2), (1), (4), (1), (2), (5),\n",
    " (1), (1), (2), (3), (2), (1), (2), (3), (1), (2)\n",
    "AS trips(passenger_count);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0cd9fa-3be3-4ee1-bf09-b529b7cdbe35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Consulta simple\n",
    "SELECT passenger_count, COUNT(*) AS viajes\n",
    "FROM trips\n",
    "GROUP BY passenger_count\n",
    "ORDER BY viajes DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f282aaa-fc50-4de5-bf94-c928b2d945df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3) Consulta de datos simples en el entorno\n",
    "Ejemplo de lectura de un CSV desde una ruta accesible (DBFS o S3). Para S3, asegúrese de contar con permisos adecuados y de haber configurado las credenciales/IAM.\n",
    "\n",
    "**Nota:** Reemplace la ruta por una válida en su entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ba5544-b338-4527-8c75-67c0c4fc3e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PySpark: lectura de CSV\n",
    "\n",
    "vol_path = \"/Volumes/jengrik_dbx_54697129984927/default/nyctaxi_vol/\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(vol_path)\n",
    "    print(\"¡Pude listar la ruta!\")\n",
    "    print(\"Archivos en el directorio: \", len(files))\n",
    "    print(files)\n",
    "except Exception as e:\n",
    "    print(\"No pude acceder a la ruta: \", e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c76df9-bf4c-41b0-9cea-e1b2c028c2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE VOLUME jengrik_dbx_54697129984927.default.nyctaxi_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc9eafc-8b0d-4d28-9fe9-753155c1c3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_csv = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(vol_path + \"trips.csv\"))\n",
    "display(df_csv.limit(2))\n",
    "\n",
    "print(df_csv.printSchema())\n",
    "print('Filas: ', df_csv.count(), \"| Columnas: \", len(df_csv.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97a6a9f-2151-48c3-89c3-686640ba2d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recapitulación de la sesión\n",
    "### Cosas para recordar\n",
    "- Databricks unifica ingeniería de datos, analítica y ML sobre Spark.\n",
    "- Lakehouse con Delta Lake aporta confiabilidad y rendimiento en el lago.\n",
    "- Workspace, clústeres y repos son pilares operativos.\n",
    "- Notebooks permiten alternar Python/SQL para productividad.\n",
    "\n",
    "### Importancia en Ingeniería de datos\n",
    "- Base para pipelines escalables y gobernados.\n",
    "- Reducción de silos y aceleración del time-to-insight.\n",
    "\n",
    "### Conclusiones\n",
    "Está listo el terreno para construir pipelines simples: ingestión, transformación y consultas analíticas que evolucionarán en sesiones posteriores hacia ML y orquestación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b169efdf-10b5-4efb-b02d-1eb76d0f56df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comprobación de conocimiento (opcional)\n",
    "1. ¿Qué ventajas ofrece Delta Lake para implementar un Lakehouse?\n",
    "2. ¿Qué diferencias clave existen entre Data Lake y Data Warehouse?\n",
    "3. ¿Cuál es el rol del clúster en la ejecución de notebooks?\n",
    "4. ¿Cómo alternar entre Python y SQL en un notebook de Databricks?\n",
    "5. ¿Qué prácticas ayudan a optimizar costos de cómputo en el trial?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6370768172668930,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fundamentos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
