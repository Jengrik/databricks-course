{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5828c03d",
   "metadata": {},
   "source": [
    "# Sesión 1: Introducción a Databricks y Fundamentos\n",
    "\n",
    "## Objetivo de la sesión\n",
    "Comprender los fundamentos del entorno Databricks, su relación con Apache Spark y el concepto de arquitectura Lakehouse. Al finalizar, podrás crear tu primer notebook, ejecutar consultas básicas y familiarizarte con la interfaz del entorno Databricks Free Edition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecbb22",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Databricks?\n",
    "Databricks es una **plataforma unificada de análisis de datos** basada en la nube, creada por los fundadores de **Apache Spark**. Facilita el trabajo colaborativo entre **ingeniería de datos, ciencia de datos y analítica**, integrando herramientas para **ingesta, transformación, machine learning y BI** en un solo lugar.\n",
    "\n",
    "### Historia y relación con Apache Spark\n",
    "- Apache Spark fue desarrollado en el AMPLab de UC Berkeley alrededor de 2010.\n",
    "- En 2013, sus creadores fundaron **Databricks** para ofrecer una plataforma gestionada y optimizada sobre Spark.\n",
    "- Spark es el motor de ejecución subyacente. Databricks añade:\n",
    "  - **Delta Lake** para confiabilidad y transacciones ACID en el data lake.\n",
    "  - **MLflow** para el seguimiento del ciclo de vida de modelos.\n",
    "  - **Databricks SQL** para consultas y dashboards.\n",
    "  - Notebooks colaborativos con control de versiones vía Git/GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d66a1d",
   "metadata": {},
   "source": [
    "## 2. Data Lake, Data Warehouse y Lakehouse\n",
    "\n",
    "| Concepto | Descripción | Limitaciones | Ejemplo |\n",
    "|---|---|---|---|\n",
    "| **Data Lake** | Repositorio de datos en crudo (estructurados y no estructurados). | Control de calidad complejo, consultas menos eficientes. | Archivos en S3, ADLS, GCS o DBFS. |\n",
    "| **Data Warehouse** | Almacén analítico optimizado para SQL y modelos tabulares. | Coste y rigidez ante datos semiestructurados/no estructurados. | Redshift, BigQuery, Snowflake. |\n",
    "| **Lakehouse** | Combina Data Lake + Data Warehouse con transacciones ACID y rendimiento analítico. | Requiere formatos/tecnologías modernas como **Delta Lake**. | Databricks Lakehouse Platform. |\n",
    "\n",
    "**Delta Lake** aporta: transacciones ACID, esquema, time travel y versionado, permitiendo construir un **Lakehouse** confiable sobre almacenamiento económico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd21e1",
   "metadata": {},
   "source": [
    "## 3. Casos de uso en la industria\n",
    "- Finanzas: detección de fraude, scoring de riesgo en tiempo real.\n",
    "- Retail: personalización, pronóstico de demanda, optimización de inventario.\n",
    "- Salud: integración clínica, analítica poblacional, modelos predictivos.\n",
    "- Energía/IoT: mantenimiento predictivo, análisis de sensores.\n",
    "- Educación: analítica de aprendizaje, retención estudiantil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491e7be",
   "metadata": {},
   "source": [
    "## 4. Entorno de trabajo en Databricks\n",
    "\n",
    "### Navegación en el Workspace\n",
    "- **Home**: página inicial del usuario.\n",
    "- **Workspace**: notebooks, scripts y carpetas del proyecto.\n",
    "- **Repos**: integración con Git/GitHub para versionado.\n",
    "- **Data**: acceso a bases y tablas, exploración de catálogos.\n",
    "- **Compute**: administración de **clusters**.\n",
    "- **SQL**: editor SQL, visualizaciones y (si aplica) dashboards.\n",
    "\n",
    "### Notebooks y lenguajes soportados\n",
    "Databricks soporta **Python, SQL, R, Scala**. Puedes fijar un lenguaje por celda con los *magics*:\n",
    "- `%python`\n",
    "- `%sql`\n",
    "- `%r`\n",
    "- `%scala`\n",
    "\n",
    "### Configuración básica de clusters (Free Edition)\n",
    "1. Ir a **Compute → Create Cluster**.\n",
    "2. Asignar un nombre (por ejemplo, `cluster_free_demo`).  \n",
    "3. Seleccionar el **Runtime** más reciente disponible (Spark 3.x).  \n",
    "4. Usar modo **Single Node** (limitación de la Free Edition).  \n",
    "5. Crear el cluster y conectarlo al notebook antes de ejecutar celdas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af99c2",
   "metadata": {},
   "source": [
    "## 5. Workshop: Primeros pasos\n",
    "\n",
    "### 5.1 Crear un notebook\n",
    "1. **Workspace → Create → Notebook**.  \n",
    "2. Elegir nombre (por ejemplo, `Introduccion_Databricks`).  \n",
    "3. Seleccionar lenguaje por defecto (**Python** o **SQL**).  \n",
    "4. Adjuntar el cluster activo al notebook.\n",
    "\n",
    "### 5.2 Ejecutar celdas\n",
    "A continuación, ejecuta las celdas de ejemplo para verificar que el entorno funciona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ce8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo básico en Python\n",
    "print(\"Hola Databricks desde Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda SQL usando magic de Databricks\n",
    "# Ejecuta esta celda en Databricks; en otros entornos, el magic %sql no estará disponible.\n",
    "\n",
    "# Databricks: descomenta la siguiente línea\n",
    "# %sql\n",
    "# SELECT \"Hola Databricks SQL\" AS mensaje;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fd393",
   "metadata": {},
   "source": [
    "### 5.3 Cargar y consultar datos simples\n",
    "\n",
    "A continuación se muestra un ejemplo con un dataset público disponible en muchos runtimes de Databricks. Si el path no existe en tu runtime de Free Edition, sustituye la ruta por un CSV propio en **DBFS**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7298f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de un CSV de ejemplo con PySpark\n",
    "# Si esta ruta no existe en tu entorno, reemplázala por un path válido en DBFS (por ejemplo: /FileStore/tables/mi_archivo.csv)\n",
    "\n",
    "csv_path = \"/databricks-datasets/learning-spark-v2/people/people-10m.csv\"\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e40b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una vista temporal y consultar con SQL (Databricks)\n",
    "# %sql\n",
    "# CREATE OR REPLACE TEMP VIEW people AS\n",
    "# SELECT * FROM csv.`/databricks-datasets/learning-spark-v2/people/people-10m.csv`;\n",
    "\n",
    "# %sql\n",
    "# SELECT firstName, lastName, gender FROM people LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f33ef0",
   "metadata": {},
   "source": [
    "## Conclusiones de la sesión\n",
    "- Databricks unifica ingeniería, analítica y ciencia de datos sobre Apache Spark.\n",
    "- El enfoque **Lakehouse** con **Delta Lake** suma confiabilidad y rendimiento analítico sobre almacenamiento de data lake.\n",
    "- La Free Edition permite prácticas individuales y versionado con Git/GitHub.\n",
    "\n",
    "## Actividad sugerida\n",
    "- Crea un notebook llamado `Exploracion_inicial`.\n",
    "- Escribe tres celdas: una en Python, otra en SQL y otra en Markdown.\n",
    "- Sube el notebook al repositorio de GitHub del curso y documenta los pasos realizados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
