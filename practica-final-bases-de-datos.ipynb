{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45146b16-8f2b-4875-bfd4-169f009b95c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Práctica integral: Bases de Datos en Databricks (Lakehouse)\n",
    "**Duración estimada:** ~2 horas  \n",
    "**Tono:** Académico y formal  \n",
    "**Entorno objetivo:** Databricks Premium Trial (AWS)  \n",
    "**Ruta base (Volumen de trabajo):** `/Volumes/jengrik_dbx_1576397358716270/default/jengrik-volume/`\n",
    "\n",
    "> Este cuaderno combina teoría y práctica para construir una base de datos de complejidad media (clientes, productos, pedidos y renglones),\n",
    "> habilitando consultas SQL, operaciones DML, _Time Travel_ de Delta y visualizaciones con Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61277fc4-f771-4220-9cab-f8d2192f8ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Requisitos previos y consideraciones de entorno\n",
    "\n",
    "### Objetivo de la sección 1\n",
    "Comprender las condiciones técnicas necesarias para ejecutar correctamente los ejercicios prácticos en Databricks Premium Trial sobre AWS.\n",
    "\n",
    "### Qué se realizará en la sección 1\n",
    "Se verificará el entorno, el clúster activo, las rutas de almacenamiento (Volumes) y las herramientas disponibles. Se introducirá el uso de celdas SQL y Python en un mismo notebook.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 1)\n",
    "Garantizar un entorno controlado es esencial para la reproducibilidad de resultados y la gestión de recursos en proyectos de ingeniería de datos distribuidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e7de59-2211-4996-99da-57fef222a063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Requisitos del entorno**  \n",
    "- **Cluster** activo (Runtime Spark 13.x+ recomendado).  \n",
    "- **Unity Catalog** y **Volumes** disponibles. Usaremos el volumen base: `/Volumes/jengrik_dbx_1576397358716270/default/jengrik-volume/` para persistir archivos “raw” (CSV/Parquet) y, opcionalmente, tablas Delta externas.  \n",
    "- Permisos adecuados de lectura/escritura sobre el volumen.\n",
    "\n",
    "**Nota sobre Databricks SQL/Notebooks**  \n",
    "- En celdas de este notebook se muestran fragmentos con magia `%sql` para ejecutar **SQL** directamente.\n",
    "- Las celdas **Python** usarán PySpark y APIs de Delta cuando aplique.\n",
    "- Si ejecutas fuera de Databricks, ignora las celdas `%sql` y replica la lógica desde Python.\n",
    "\n",
    "### Cosas para recordar\n",
    "- Un **Volume** es un espacio de archivos gobernado por Unity Catalog.\n",
    "- Un **Schema (Database)** es un contenedor lógico de tablas/vistas.\n",
    "- Puedes tener **tablas manejadas** o **tablas externas**.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Distinguir almacenamiento **lógico** del **físico** es clave para la gobernanza y la reproducibilidad.\n",
    "- Los Volumes facilitan el patrón **medallion** (raw/bronze, silver, gold).\n",
    "\n",
    "### Conclusiones\n",
    "- Preparar el entorno y permisos reduce fricción técnica durante la práctica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f880f77-d49f-4c1f-b95f-b21e9845f8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Bases de datos lógicas vs. almacenamiento físico\n",
    "\n",
    "### Objetivo de la sección 2\n",
    "Distinguir los conceptos de base de datos, esquema, tabla lógica y almacenamiento físico dentro del modelo Lakehouse de Databricks.\n",
    "\n",
    "### Qué se realizará en la sección 2\n",
    "Se analizará cómo las bases de datos y tablas lógicas en el catálogo se relacionan con los archivos reales ubicados en Volumes o S3.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 2)\n",
    "Comprender esta relación permite diseñar arquitecturas gobernadas, donde los metadatos y los datos pueden evolucionar de forma independiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5371ba9a-aee9-4576-99d4-5ef048b726ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bases de datos lógicas (catálogo/esquema):**  \n",
    "- Estructuran y gobiernan **metadatos** (nombres, columnas, permisos, lineage).  \n",
    "- Separan la **vista lógica** del **lugar físico** donde residen los datos.\n",
    "\n",
    "**Almacenamiento físico (Volumes / S3 / External Locations):**  \n",
    "- Contiene los **archivos reales**: Delta, Parquet, CSV, etc.  \n",
    "- La misma tabla lógica puede apuntar a diferentes ubicaciones físicas a lo largo del tiempo.\n",
    "\n",
    "**Tablas manejadas vs. externas:**  \n",
    "- **Manejadas**: El motor controla el _path_ y ciclo de vida de los archivos.  \n",
    "- **Externas**: El metastore referencia un _path_ explícito. Tú gestionas el ciclo de vida de los archivos.\n",
    "\n",
    "### Cosas para recordar\n",
    "- No toda tabla Delta es manejada; depende de cómo fue creada.  \n",
    "- El **catálogo** registra lo que hay; los **archivos** son la verdad última de los datos.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Desacopla la evolución del esquema lógico del repositorio físico.\n",
    "- Habilita **gobernanza** (ACLs, lineage) sin perder flexibilidad operativa.\n",
    "\n",
    "### Conclusiones\n",
    "- El Lakehouse depende de una relación clara entre **metadatos** (catálogo) y **datos** (storage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f397a34b-cd78-4bc6-bb49-d946b3eb04c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Creación de esquema y preparación del entorno\n",
    "\n",
    "### Objetivo de la sección 3\n",
    "Aprender a crear y utilizar esquemas (bases de datos) en Databricks para organizar objetos lógicos y físicos.\n",
    "\n",
    "### Qué se realizará en la sección 3\n",
    "Se usará el comando `CREATE SCHEMA` y se definirá el esquema activo mediante `USE`. También se parametrizará la ruta base del volumen de trabajo.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 3)\n",
    "Definir correctamente el esquema garantiza aislamiento entre ambientes y facilita la trazabilidad de los objetos creados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6119335-67cf-4879-86b9-15e16c50f1c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS demo_db_jengrik;\n",
    "USE demo_db_jengrik;\n",
    "\n",
    "-- Verificar el esquema actual\n",
    "SELECT current_catalog(), current_schema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc26d403-9387-44f6-8bfe-3a09a273291d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: variables base (útil para parametrizar rutas)\n",
    "volume_base = \"/Volumes/jengrik_dbx_1576397358716270/default/jengrik-volume/\"\n",
    "db_name = \"demo_db_jengrik\"\n",
    "\n",
    "print(\"Used Volume Base\", volume_base)\n",
    "print(\"Used Database Name\", db_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a43aa0-d7e0-4198-8f87-5568d79947af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- `CREATE SCHEMA` crea un contenedor lógico.  \n",
    "- `USE <schema>` cambia el contexto de ejecución SQL.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Separar ambientes y dominios de negocio por esquemas facilita el **aislamiento** y la **gestión de permisos**.\n",
    "\n",
    "### Conclusiones\n",
    "- Un esquema dedicado a la práctica evita colisiones y mejora la trazabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e907c74-d092-42e3-929e-a37415a59bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Generación de datos sintéticos y creación de tablas Delta\n",
    "\n",
    "### Objetivo de la sección 4\n",
    "Construir un conjunto de datos de tamaño mediano para simular una base de datos empresarial.\n",
    "\n",
    "### Qué se realizará en la sección 4\n",
    "Se generarán datos de clientes, productos, pedidos y renglones, se guardarán en formato CSV/Parquet (zona raw) y luego se crearán tablas Delta manejadas.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 4)\n",
    "Esta práctica permite comprender el flujo de ingestión y consolidación, así como el rol de Delta Lake en la confiabilidad de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443bad89-8e8d-46a1-ac9d-12b986f0ab11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Generar datos sintéticos 'medianos' y persistir archivos raw (CSV/Parquet) en el Volume.\n",
    "# Luego, crear tablas Delta manejadas a partir de esos datos.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Row\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "spark.sql(\"USE {}\".format(db_name))\n",
    "\n",
    "random.seed(42) # Semilla de Reproducibilidad\n",
    "\n",
    "cities = [\"Bogotá\", \"Medellín\", \"Cali\", \"Barranquilla\", \"Bucaramanga\", \"Cartagena\"]\n",
    "categories = [\"Electrónica\", \"Hogar\", \"Deportes\", \"Moda\", \"Juguetes\"]\n",
    "products = []\n",
    "\n",
    "for pid in range(1001, 1021):\n",
    "    # Para 20 productos\n",
    "    products.append(Row(\n",
    "        producto_id = pid,\n",
    "        name = \"Producto {}\".format(pid),\n",
    "        category = random.choice(categories),\n",
    "        price = round(random.uniform(20, 500), 2)\n",
    "    ))\n",
    "\n",
    "customers = []\n",
    "for cid in range(1, 301):\n",
    "    # Para 300 clientes\n",
    "    customers.append(Row(\n",
    "        customer_id = cid,\n",
    "        full_name = \"Cliente {:03d}\".format(cid),\n",
    "        city = random.choice(cities),\n",
    "        signup_date = (datetime.date(2023,1,1) + datetime.timedelta(days=random.randint(0, 600))).isoformat()\n",
    "    ))\n",
    "\n",
    "# Orders\n",
    "orders = []\n",
    "order_items = []\n",
    "order_id_seq = 1\n",
    "\n",
    "start_date = datetime.date(2024,1,1)\n",
    "days_span = 500 # Mas o menos a mediados del 2025\n",
    "\n",
    "for _ in range(2000):\n",
    "    # Para 2000 ordenes\n",
    "    cust_id = random.randint(1, 300)\n",
    "    order_date = start_date + datetime.timedelta(days=random.randint(0, days_span))\n",
    "    # 1 a 5 productos por orden exitosa\n",
    "    num_items = random.randint(1, 5)\n",
    "    subtotal = 0.0\n",
    "    for i in range(num_items):\n",
    "       # De 1 a 5 productos, cantidad aleatoria\n",
    "       prod = random.choice(products)\n",
    "       qty = random.randint(1, 4)\n",
    "       item_total = prod['price']*qty\n",
    "       order_items.append(Row(\n",
    "           order_id = order_id_seq,\n",
    "           customer_id = cust_id,\n",
    "           order_date = order_date.isoformat(),\n",
    "           order_total = round(subtotal, 2)\n",
    "        ))\n",
    "       subtotal += item_total\n",
    "    orders.append(Row(\n",
    "        order_id = order_id_seq,\n",
    "        customer_id = cust_id,\n",
    "        order_date = order_date.isoformat(),\n",
    "        order_total = round(subtotal, 2)\n",
    "    ))\n",
    "    order_id_seq += 1\n",
    "\n",
    "# Creación de DataFrames\n",
    "df_products = spark.createDataFrame(products)\n",
    "df_customers = spark.createDataFrame(customers)\n",
    "df_orders = spark.createDataFrame(orders)\n",
    "df_order_items = spark.createDataFrame(order_items)\n",
    "\n",
    "# Creación del RAW ( para persistencia )\n",
    "\n",
    "raw_path = volume_base.rstrip(\"/\") + \"/raw/\"\n",
    "(\n",
    "    df_products.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(raw_path + \"products_csv/\")\n",
    ")\n",
    "(\n",
    "    df_customers.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(raw_path + \"customers_csv/\")\n",
    ")\n",
    "\n",
    "df_orders.write.mode(\"overwrite\").parquet(raw_path + \"orders_parquet/\")\n",
    "df_order_items.write.mode(\"overwrite\").parquet(raw_path + \"order_items_parquet/\")\n",
    "\n",
    "print(\"Los RAW han sido creados en el Volumen, la dirección es: \", raw_path)\n",
    "\n",
    "# Creación y Escritura de las Tablas Delta\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products\")\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"orders\")\n",
    "df_order_items.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"order_items\")\n",
    "\n",
    "print(\"Tablas Delta [ Managed ] creadas en el schema: \", db_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d32b688-1326-4893-b8e4-7be7329236fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- Separar **RAW (archivos)** de **tablas Delta** facilita el linaje y auditoría.\n",
    "- `saveAsTable` crea tablas **manejadas** por el metastore.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Disponer de datos “medianos” habilita **agregaciones** y **gráficos** con significado.\n",
    "- CSV y Parquet como formatos de **ingesta**; Delta como formato de **consumo**.\n",
    "\n",
    "### Conclusiones\n",
    "- Partir de archivos RAW y consolidar en Delta es un patrón común (Bronze → Silver).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40342e5e-73df-47dc-b8d3-c21133fa0ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Consultas SQL y agregaciones analíticas\n",
    "\n",
    "### Objetivo de la sección 5\n",
    "Aplicar consultas SQL sobre tablas Delta para obtener información de valor analítico.\n",
    "\n",
    "### Qué se realizará en la sección 5\n",
    "Se ejecutarán consultas de exploración, uniones entre tablas de hechos y dimensiones, y agregaciones como ventas por ciudad o categoría.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 5)\n",
    "Ejercitar el lenguaje SQL dentro de Databricks refuerza la integración entre análisis y almacenamiento distribuido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09f59c9-489b-48bb-acf3-e7284e3d64f6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":114},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760978572441}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE demo_db_jengrik;\n",
    "\n",
    "-- Inspección Rápida\n",
    " SHOW TABLES;\n",
    "\n",
    "-- Muestras\n",
    "SELECT * FROM products LIMIT 5;\n",
    "SELECT * FROM customers LIMIT 5;\n",
    "SELECT * FROM orders LIMIT 5;\n",
    "SELECT * FROM order_items LIMIT 5;\n",
    "\n",
    "-- Ventas por Ciudad ( Uniendo el Cliente con los Pedidos )\n",
    "SELECT c.city, SUM(o.order_total) AS ventas_ciudad\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "GROUP BY c.city\n",
    "ORDER BY ventas_ciudad DESC;\n",
    "\n",
    "-- TOP 10 productos por venta\n",
    "SELECT p.name, p.category, ROUND(SUM(oi.order_total),2) AS ventas_producto\n",
    "FROM order_items oi\n",
    "JOIN products p ON oi.order_id = p.producto_id\n",
    "GROUP BY p.name, p.category\n",
    "ORDER BY ventas_producto DESC\n",
    "LIMIT 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c057d20-758a-43a6-ba47-569365a9ed54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- Mantén **claves sustitutas** simples (IDs enteros) para joins eficientes.\n",
    "- Prefiere agregaciones sobre **order_items** para granularidad de ventas.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Las uniones entre dimensiones y hechos son el corazón del **modelo analítico**.\n",
    "- Diseños claros reducen costos de computación y errores de calidad.\n",
    "\n",
    "### Conclusiones\n",
    "- Con un modelo sencillo ya podemos responder preguntas de negocio relevantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c4ba2b-91d0-481d-890d-443fedc9d16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Operaciones DML e introducción a vistas temporales\n",
    "\n",
    "### Objetivo de la sección 6\n",
    "Explorar las operaciones de manipulación de datos (INSERT, UPDATE) y la creación de vistas temporales para análisis ad-hoc.\n",
    "\n",
    "### Qué se realizará en la sección 6\n",
    "Se insertarán nuevos registros, se modificarán valores existentes y se definirán vistas temporales basadas en criterios de negocio.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 6)\n",
    "Estas operaciones permiten validar hipótesis analíticas y mantener entornos de prueba controlados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf80c79-fdca-432e-8c41-1841f3b4dc07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE demo_db_jengrik;\n",
    "\n",
    "-- Inersción de un Nuevo Cliente\n",
    "INSERT INTO customers (customer_id, full_name, city, signup_date)\n",
    "VALUES (9999, 'Juan Perez', 'New York', '2023-01-01')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e340ca-11c1-484b-b11f-ec44a7e48baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- `INSERT` y `UPDATE` son soportados por Delta Lake.\n",
    "- Las **vistas temporales** viven dentro de la sesión del cluster.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Las operaciones DML controladas ayudan a **simular escenarios** y validar reglas de negocio.\n",
    "- Las vistas permiten **encapsular** lógicas de análisis sin materializar tablas.\n",
    "\n",
    "### Conclusiones\n",
    "- El flujo _ingesta → DML → vistas_ es una base sólida para análisis y prototipos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba48629e-e033-488e-9b0e-cc35744030e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Versionado y Time Travel en Delta Lake\n",
    "\n",
    "### Objetivo de la sección 7\n",
    "Comprender cómo Delta Lake registra automáticamente versiones de cada escritura y cómo acceder a ellas.\n",
    "\n",
    "### Qué se realizará en la sección 7\n",
    "Se examinará el historial de una tabla Delta con `DESCRIBE HISTORY` y se realizarán consultas a versiones anteriores.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 7)\n",
    "El Time Travel es esencial para auditoría, depuración y reproducibilidad de resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384f6064-a725-4781-9438-e6907fc4d139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE demo_db_jengrik;\n",
    "\n",
    "-- Historico Delta\n",
    "DESCRIBE HISTORY products;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2174c47f-fcea-4699-9932-8ec3ffd0c807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- Cada escritura en una tabla Delta crea una **versión**.\n",
    "- `DESCRIBE HISTORY` revela operaciones, _user_, _timestamp_, _operationMetrics_.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- **Auditoría** y **reproducibilidad**: volver a un estado anterior para depuraciones o comparativas históricas.\n",
    "\n",
    "### Conclusiones\n",
    "- _Time Travel_ fortalece la **confiabilidad** y acelera el **debugging** en pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12555e9f-ab9b-4c62-8606-13528381c687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Visualización y análisis gráfico con Python\n",
    "\n",
    "### Objetivo de la sección 8\n",
    "Integrar consultas SQL con visualizaciones en Python para interpretar resultados de manera visual y comprensible.\n",
    "\n",
    "### Qué se realizará en la sección 8\n",
    "Se utilizará matplotlib para graficar ventas diarias, por ciudad y por categoría a partir de los datos generados.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 8)\n",
    "La visualización de datos es clave para detectar patrones, anomalías y validar supuestos analíticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2991f1fd-e98d-456d-8687-63e1ada54016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Visualizaciones básicas (usar matplotlib, sin estilos ni subplots).\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark.sql(\"USE {}\".format(db_name))\n",
    "\n",
    "ventas_por_fecha = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  order_date,\n",
    "  SUM(order_total) AS ventas_diarias\n",
    "FROM orders\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date                             \n",
    "\"\"\").toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ventas_por_fecha['order_date'], ventas_por_fecha['ventas_diarias'])\n",
    "plt.title(\"Ventas Diarias\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Ventas\")\n",
    "plt.tick_params(axis='x', rotation=45)\n",
    "plt.show()\n",
    "\n",
    "ventas_por_ciudad = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    c.city,\n",
    "    SUM(o.order_total) AS ventas_ciudad\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "GROUP BY c.city\n",
    "ORDER BY ventas_ciudad DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(ventas_por_ciudad['city'], ventas_por_ciudad['ventas_ciudad'])\n",
    "plt.title(\"Ventas por Ciudad\")\n",
    "plt.xlabel(\"Ciudad\")\n",
    "plt.ylabel(\"Ventas\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d5fbdc-d58d-45b7-8521-5abd692aed2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- Las visualizaciones deben responder preguntas concretas.\n",
    "- Evita gráficos redundantes; prioriza claridad y legibilidad.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Validaciones visuales de tendencias y outliers.\n",
    "- Complementa el SQL con exploración visual.\n",
    "\n",
    "### Conclusiones\n",
    "- Series temporales y barras por dimensión aportan valor inmediato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd03e9f4-ced8-419b-8147-4fbecb7aa47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Creación de tablas Delta externas\n",
    "\n",
    "### Objetivo de la sección 9\n",
    "Aprender a registrar tablas Delta que apuntan a rutas explícitas dentro de Volumes.\n",
    "\n",
    "### Qué se realizará en la sección 9\n",
    "Se materializarán tablas Delta externas a partir de las existentes y se registrarán con `USING DELTA LOCATION`.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 9)\n",
    "Las tablas externas son útiles para interoperabilidad o cuando la organización gestiona el almacenamiento físico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39f07b03-5565-4ee1-9c4c-d710f72aa826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks: ejecutar como SQL\n",
    "%sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e36a2e36-2fbb-4ee5-b457-205c4416f6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: materializar Delta externo en Volume y registrar tabla externa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c8b9ac-97a2-4468-a0d3-74407ecc664c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks: ejecutar como SQL\n",
    "%sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7b5cb7b-bd4f-469b-a98e-83178349f13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosas para recordar\n",
    "- `USING DELTA LOCATION` registra tablas **externas** referenciando rutas explícitas.\n",
    "- La **vida** de los datos está desacoplada de la vida del objeto lógico en el metastore.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- Útil para interoperar con otras herramientas o cuando el storage se gestiona fuera de Databricks.\n",
    "\n",
    "### Conclusiones\n",
    "- Tablas externas aportan flexibilidad; requieren disciplina en housekeeping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a94180-08c7-483c-9054-78f17ed6be97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Reflexiones adicionales (evaluación formativa)\n",
    "\n",
    "### Objetivo de la sección 10\n",
    "Consolidar los aprendizajes conceptuales y prácticos mediante preguntas de análisis y reflexión.\n",
    "\n",
    "### Qué se realizará en la sección 10\n",
    "Se propondrán ejercicios orientados a evaluar decisiones arquitectónicas, calidad de datos y aplicación de Time Travel.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 10)\n",
    "La evaluación formativa refuerza la comprensión crítica de fundamentos de ingeniería de datos y su aplicación práctica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec34a44-ff4e-4514-8df2-f71eaf0dde86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. ¿En qué escenarios conviene preferir **tablas manejadas** frente a **externas**? Justifique con criterios de gobierno y operación.\n",
    "2. Diseñe un **índice de calidad de datos** (simple) para `orders` y proponga validaciones mínimas (nulos, rangos, duplicados).\n",
    "3. Elabore un pequeño **experimento** de _Time Travel_ que muestre la trazabilidad de un cambio de precio en `products`.\n",
    "4. Pregunta abierta: ¿Qué **métricas de negocio** adicionales calcularía (margen, ticket promedio, recurrencia de compra) y cómo validaría su consistencia?\n",
    "\n",
    "### Cosas para recordar\n",
    "- La calidad de datos y la trazabilidad impactan directamente la **confiabilidad** del análisis.\n",
    "- Las decisiones de modelado y gobierno deben alinearse con el **dominio de negocio**.\n",
    "\n",
    "### Importancia en Ingeniería de Datos\n",
    "- La evaluación formativa promueve pensamiento crítico sobre **trade-offs** arquitectónicos.\n",
    "- Refuerza buenas prácticas de **gobernanza** y **observabilidad**.\n",
    "\n",
    "### Conclusiones\n",
    "- Más allá del código, el valor proviene de interpretaciones sólidas y decisiones informadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e7cabd6-02d7-49a2-80ee-b40c1309748b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Cierre y proyección profesional\n",
    "\n",
    "### Objetivo de la sección 11\n",
    "Sintetizar los conocimientos adquiridos sobre bases de datos, Delta Lake y buenas prácticas en Databricks.\n",
    "\n",
    "### Qué se realizará en la sección 11\n",
    "Se revisarán los pasos ejecutados, se identificarán oportunidades de mejora y se propondrán líneas de trabajo futuras.\n",
    "\n",
    "### Por qué es importante hacerlo (sección 11)\n",
    "El cierre permite consolidar aprendizajes y conectar la práctica con procesos reales de automatización y gobernanza de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2611ad35-b0f7-40fa-8044-af544582884e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hemos construido una base de datos de tamaño mediano con un modelo lógico claro y datos suficientes para análisis y visualización.\n",
    "Se cubrieron: creación de esquema, ingesta de datos, consolidación en Delta, consultas SQL, DML, Time Travel, tablas externas y gráficos.\n",
    "\n",
    "**Siguiente paso sugerido:** Integrar este cuaderno a un **repositorio Git** y preparar un **job** programado para refrescar datos y generar reportes periódicos.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6370768172668997,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "practica-final-bases-de-datos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
